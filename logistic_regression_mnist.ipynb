{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "inputs, hiddens, outputs = 784, 200, 10\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "#compose several transformations together \n",
    "#image to tensor, normalization for the given channels (mean = 0.13, std = 0.3)\n",
    "transformation = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "#dataset for training (download = True to download the dataset for the first time)\n",
    "train_dataset = datasets.MNIST('mnist/',train=True,transform=transformation, download=False)\n",
    "#loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#dataset for testing (download = True to download the dataset for the first time)\n",
    "test_dataset = datasets.MNIST('mnist/',train=False,transform=transformation, download=False)\n",
    "#loader\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "#the train dataset is comprised by 60000 images\n",
    "print (len(train_dataset))\n",
    "#its loader posseses batches of 20 images\n",
    "print (len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_Note_**:  train_dataset has a transformation applied. Originally, every element is a tuple of a PIL image and its numeric label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 28, 28])\n",
      "0\n",
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QeCIvxAUIQfCIrwA0H9H4BpmwJXvvG+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13466a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "#every element of train_dataset is a tuple conformed by a tensor (the image) and its label\n",
    "print(train_dataset[0][0].dim()) #image\n",
    "print(train_dataset[0][0].size())\n",
    "print(train_dataset[0][1].dim()) #label (the corresponding number)\n",
    "print(train_dataset[0][1].size())\n",
    "\n",
    "plt.imshow(train_dataset[0][0][0], cmap = 'gray' )\n",
    "plt.show()\n",
    "\n",
    "print(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping an element of train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "img = train_dataset[0][0]\n",
    "#making img into a 1d tensor\n",
    "reshaped_img = img.view(-1, inputs)     #-1 infers the correct size given the other dimensions (inputs)\n",
    "print(reshaped_img.size())\n",
    "print(reshaped_img.dim())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "#every batch has 20 images and 20 labels\n",
    "batch = batches[0]\n",
    "print( batch[0].size() )\n",
    "print( batch[1].size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden_layer = nn.Linear(inputs, hiddens)\n",
    "        self.output_layer = nn.Linear(hiddens, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sigmoid(self.hidden_layer(x))\n",
    "        out = self.sigmoid(self.output_layer(out))\n",
    "        return out\n",
    "\n",
    "    def name(self):\n",
    "        return \"mlp\"\n",
    "\n",
    "def train():\n",
    "    model = mlp()\n",
    "    loss = nn.MSELoss(size_average=False)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_error = 0\n",
    "        #for every image in train_loader\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            images = Variable(images.view(-1, inputs))\n",
    "            # Convert class label to one hot vector \n",
    "            one_hot = torch.FloatTensor(labels.size(0), 10).zero_()\n",
    "            target = one_hot.scatter_(1, labels.view((labels.size(0),1)), 1)            \n",
    "            target = Variable(target)\n",
    "            # Compute loss and gradient\n",
    "            optimizer.zero_grad()\n",
    "            out = model(images)\n",
    "            error = loss(out, target)\n",
    "            error.backward()\n",
    "            # Apply gradient\n",
    "            optimizer.step()\n",
    "            avg_error += error.data[0]\n",
    "        # Average cost for epoch (over all training dataset samples)\n",
    "        avg_error /= train_loader.dataset.train_data.shape[0]\n",
    "        #print (\"Epoch [%d/%d], error: %.4f\" %(epoch+1, epochs, avg_error))\n",
    "    # Save model to file\n",
    "    torch.save(model.state_dict(), 'model.pkl')\n",
    "\n",
    "def predict_all():\n",
    "    model = mlp()\n",
    "    model.load_state_dict(torch.load('model.pkl'))\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, inputs))\n",
    "        out = model(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('accuracy: %0.2f %%' % (100.0 * correct / total))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading a model and classifying an image from the test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1894ae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my prediction is 5 and the actual digit was 5 \n"
     ]
    }
   ],
   "source": [
    "model = mlp()\n",
    "model.load_state_dict(torch.load('model.pkl'))\n",
    "\n",
    "i = random.randint(0,10000)\n",
    "img = test_dataset[i][0]\n",
    "reshaped_img = img.view(-1, inputs)\n",
    "out = model(reshaped_img)\n",
    "_,predicted = torch.max(out,1)\n",
    "\n",
    "plt.imshow(img[0], cmap = 'gray' )\n",
    "plt.show()\n",
    "\n",
    "print(\"my prediction is {} and the actual digit was {} \".format(predicted[0], test_dataset[i][1]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
