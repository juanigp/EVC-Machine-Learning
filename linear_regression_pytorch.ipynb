{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "#Dataset\n",
    "\"\"\"\n",
    "x_train = torch.tensor([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "[10.791], [5.313], [7.997], [3.1]])\n",
    "\n",
    "y_train = torch.tensor([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "[3.465], [1.65], [2.904], [1.3]])\n",
    "\"\"\"\n",
    "x_train = torch.rand(30, input_size).type(torch.FloatTensor)\n",
    "y_train = torch.rand(30, output_size).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "#linear regression model. Inherits nn.Module\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #nn.Module.__init__(self)\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #linear layer: applies a linear transformation to incoming data\n",
    "        self.linear = nn.Linear(input_size, output_size) \n",
    "        \n",
    "    #defines the computation performed at every call\n",
    "    def forward(self, x):\n",
    "        #apply linear transformation\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "#Loss and Optimizer\n",
    "#creates a criterion that measures the mean squared error \n",
    "#between n elements in the input x and target y\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer, Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juanig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], Loss: 0.0931\n",
      "Epoch [10/1000], Loss: 0.0929\n",
      "Epoch [15/1000], Loss: 0.0926\n",
      "Epoch [20/1000], Loss: 0.0924\n",
      "Epoch [25/1000], Loss: 0.0922\n",
      "Epoch [30/1000], Loss: 0.0920\n",
      "Epoch [35/1000], Loss: 0.0917\n",
      "Epoch [40/1000], Loss: 0.0915\n",
      "Epoch [45/1000], Loss: 0.0913\n",
      "Epoch [50/1000], Loss: 0.0911\n",
      "Epoch [55/1000], Loss: 0.0909\n",
      "Epoch [60/1000], Loss: 0.0907\n",
      "Epoch [65/1000], Loss: 0.0906\n",
      "Epoch [70/1000], Loss: 0.0904\n",
      "Epoch [75/1000], Loss: 0.0902\n",
      "Epoch [80/1000], Loss: 0.0900\n",
      "Epoch [85/1000], Loss: 0.0899\n",
      "Epoch [90/1000], Loss: 0.0897\n",
      "Epoch [95/1000], Loss: 0.0895\n",
      "Epoch [100/1000], Loss: 0.0894\n",
      "Epoch [105/1000], Loss: 0.0892\n",
      "Epoch [110/1000], Loss: 0.0891\n",
      "Epoch [115/1000], Loss: 0.0889\n",
      "Epoch [120/1000], Loss: 0.0888\n",
      "Epoch [125/1000], Loss: 0.0886\n",
      "Epoch [130/1000], Loss: 0.0885\n",
      "Epoch [135/1000], Loss: 0.0883\n",
      "Epoch [140/1000], Loss: 0.0882\n",
      "Epoch [145/1000], Loss: 0.0881\n",
      "Epoch [150/1000], Loss: 0.0880\n",
      "Epoch [155/1000], Loss: 0.0878\n",
      "Epoch [160/1000], Loss: 0.0877\n",
      "Epoch [165/1000], Loss: 0.0876\n",
      "Epoch [170/1000], Loss: 0.0875\n",
      "Epoch [175/1000], Loss: 0.0874\n",
      "Epoch [180/1000], Loss: 0.0872\n",
      "Epoch [185/1000], Loss: 0.0871\n",
      "Epoch [190/1000], Loss: 0.0870\n",
      "Epoch [195/1000], Loss: 0.0869\n",
      "Epoch [200/1000], Loss: 0.0868\n",
      "Epoch [205/1000], Loss: 0.0867\n",
      "Epoch [210/1000], Loss: 0.0866\n",
      "Epoch [215/1000], Loss: 0.0865\n",
      "Epoch [220/1000], Loss: 0.0864\n",
      "Epoch [225/1000], Loss: 0.0863\n",
      "Epoch [230/1000], Loss: 0.0863\n",
      "Epoch [235/1000], Loss: 0.0862\n",
      "Epoch [240/1000], Loss: 0.0861\n",
      "Epoch [245/1000], Loss: 0.0860\n",
      "Epoch [250/1000], Loss: 0.0859\n",
      "Epoch [255/1000], Loss: 0.0858\n",
      "Epoch [260/1000], Loss: 0.0858\n",
      "Epoch [265/1000], Loss: 0.0857\n",
      "Epoch [270/1000], Loss: 0.0856\n",
      "Epoch [275/1000], Loss: 0.0855\n",
      "Epoch [280/1000], Loss: 0.0855\n",
      "Epoch [285/1000], Loss: 0.0854\n",
      "Epoch [290/1000], Loss: 0.0853\n",
      "Epoch [295/1000], Loss: 0.0852\n",
      "Epoch [300/1000], Loss: 0.0852\n",
      "Epoch [305/1000], Loss: 0.0851\n",
      "Epoch [310/1000], Loss: 0.0851\n",
      "Epoch [315/1000], Loss: 0.0850\n",
      "Epoch [320/1000], Loss: 0.0849\n",
      "Epoch [325/1000], Loss: 0.0849\n",
      "Epoch [330/1000], Loss: 0.0848\n",
      "Epoch [335/1000], Loss: 0.0848\n",
      "Epoch [340/1000], Loss: 0.0847\n",
      "Epoch [345/1000], Loss: 0.0846\n",
      "Epoch [350/1000], Loss: 0.0846\n",
      "Epoch [355/1000], Loss: 0.0845\n",
      "Epoch [360/1000], Loss: 0.0845\n",
      "Epoch [365/1000], Loss: 0.0844\n",
      "Epoch [370/1000], Loss: 0.0844\n",
      "Epoch [375/1000], Loss: 0.0843\n",
      "Epoch [380/1000], Loss: 0.0843\n",
      "Epoch [385/1000], Loss: 0.0842\n",
      "Epoch [390/1000], Loss: 0.0842\n",
      "Epoch [395/1000], Loss: 0.0841\n",
      "Epoch [400/1000], Loss: 0.0841\n",
      "Epoch [405/1000], Loss: 0.0841\n",
      "Epoch [410/1000], Loss: 0.0840\n",
      "Epoch [415/1000], Loss: 0.0840\n",
      "Epoch [420/1000], Loss: 0.0839\n",
      "Epoch [425/1000], Loss: 0.0839\n",
      "Epoch [430/1000], Loss: 0.0839\n",
      "Epoch [435/1000], Loss: 0.0838\n",
      "Epoch [440/1000], Loss: 0.0838\n",
      "Epoch [445/1000], Loss: 0.0837\n",
      "Epoch [450/1000], Loss: 0.0837\n",
      "Epoch [455/1000], Loss: 0.0837\n",
      "Epoch [460/1000], Loss: 0.0836\n",
      "Epoch [465/1000], Loss: 0.0836\n",
      "Epoch [470/1000], Loss: 0.0836\n",
      "Epoch [475/1000], Loss: 0.0835\n",
      "Epoch [480/1000], Loss: 0.0835\n",
      "Epoch [485/1000], Loss: 0.0835\n",
      "Epoch [490/1000], Loss: 0.0834\n",
      "Epoch [495/1000], Loss: 0.0834\n",
      "Epoch [500/1000], Loss: 0.0834\n",
      "Epoch [505/1000], Loss: 0.0834\n",
      "Epoch [510/1000], Loss: 0.0833\n",
      "Epoch [515/1000], Loss: 0.0833\n",
      "Epoch [520/1000], Loss: 0.0833\n",
      "Epoch [525/1000], Loss: 0.0832\n",
      "Epoch [530/1000], Loss: 0.0832\n",
      "Epoch [535/1000], Loss: 0.0832\n",
      "Epoch [540/1000], Loss: 0.0832\n",
      "Epoch [545/1000], Loss: 0.0831\n",
      "Epoch [550/1000], Loss: 0.0831\n",
      "Epoch [555/1000], Loss: 0.0831\n",
      "Epoch [560/1000], Loss: 0.0831\n",
      "Epoch [565/1000], Loss: 0.0830\n",
      "Epoch [570/1000], Loss: 0.0830\n",
      "Epoch [575/1000], Loss: 0.0830\n",
      "Epoch [580/1000], Loss: 0.0830\n",
      "Epoch [585/1000], Loss: 0.0829\n",
      "Epoch [590/1000], Loss: 0.0829\n",
      "Epoch [595/1000], Loss: 0.0829\n",
      "Epoch [600/1000], Loss: 0.0829\n",
      "Epoch [605/1000], Loss: 0.0829\n",
      "Epoch [610/1000], Loss: 0.0828\n",
      "Epoch [615/1000], Loss: 0.0828\n",
      "Epoch [620/1000], Loss: 0.0828\n",
      "Epoch [625/1000], Loss: 0.0828\n",
      "Epoch [630/1000], Loss: 0.0828\n",
      "Epoch [635/1000], Loss: 0.0827\n",
      "Epoch [640/1000], Loss: 0.0827\n",
      "Epoch [645/1000], Loss: 0.0827\n",
      "Epoch [650/1000], Loss: 0.0827\n",
      "Epoch [655/1000], Loss: 0.0827\n",
      "Epoch [660/1000], Loss: 0.0827\n",
      "Epoch [665/1000], Loss: 0.0826\n",
      "Epoch [670/1000], Loss: 0.0826\n",
      "Epoch [675/1000], Loss: 0.0826\n",
      "Epoch [680/1000], Loss: 0.0826\n",
      "Epoch [685/1000], Loss: 0.0826\n",
      "Epoch [690/1000], Loss: 0.0826\n",
      "Epoch [695/1000], Loss: 0.0825\n",
      "Epoch [700/1000], Loss: 0.0825\n",
      "Epoch [705/1000], Loss: 0.0825\n",
      "Epoch [710/1000], Loss: 0.0825\n",
      "Epoch [715/1000], Loss: 0.0825\n",
      "Epoch [720/1000], Loss: 0.0825\n",
      "Epoch [725/1000], Loss: 0.0825\n",
      "Epoch [730/1000], Loss: 0.0824\n",
      "Epoch [735/1000], Loss: 0.0824\n",
      "Epoch [740/1000], Loss: 0.0824\n",
      "Epoch [745/1000], Loss: 0.0824\n",
      "Epoch [750/1000], Loss: 0.0824\n",
      "Epoch [755/1000], Loss: 0.0824\n",
      "Epoch [760/1000], Loss: 0.0824\n",
      "Epoch [765/1000], Loss: 0.0824\n",
      "Epoch [770/1000], Loss: 0.0823\n",
      "Epoch [775/1000], Loss: 0.0823\n",
      "Epoch [780/1000], Loss: 0.0823\n",
      "Epoch [785/1000], Loss: 0.0823\n",
      "Epoch [790/1000], Loss: 0.0823\n",
      "Epoch [795/1000], Loss: 0.0823\n",
      "Epoch [800/1000], Loss: 0.0823\n",
      "Epoch [805/1000], Loss: 0.0823\n",
      "Epoch [810/1000], Loss: 0.0822\n",
      "Epoch [815/1000], Loss: 0.0822\n",
      "Epoch [820/1000], Loss: 0.0822\n",
      "Epoch [825/1000], Loss: 0.0822\n",
      "Epoch [830/1000], Loss: 0.0822\n",
      "Epoch [835/1000], Loss: 0.0822\n",
      "Epoch [840/1000], Loss: 0.0822\n",
      "Epoch [845/1000], Loss: 0.0822\n",
      "Epoch [850/1000], Loss: 0.0822\n",
      "Epoch [855/1000], Loss: 0.0821\n",
      "Epoch [860/1000], Loss: 0.0821\n",
      "Epoch [865/1000], Loss: 0.0821\n",
      "Epoch [870/1000], Loss: 0.0821\n",
      "Epoch [875/1000], Loss: 0.0821\n",
      "Epoch [880/1000], Loss: 0.0821\n",
      "Epoch [885/1000], Loss: 0.0821\n",
      "Epoch [890/1000], Loss: 0.0821\n",
      "Epoch [895/1000], Loss: 0.0821\n",
      "Epoch [900/1000], Loss: 0.0821\n",
      "Epoch [905/1000], Loss: 0.0821\n",
      "Epoch [910/1000], Loss: 0.0820\n",
      "Epoch [915/1000], Loss: 0.0820\n",
      "Epoch [920/1000], Loss: 0.0820\n",
      "Epoch [925/1000], Loss: 0.0820\n",
      "Epoch [930/1000], Loss: 0.0820\n",
      "Epoch [935/1000], Loss: 0.0820\n",
      "Epoch [940/1000], Loss: 0.0820\n",
      "Epoch [945/1000], Loss: 0.0820\n",
      "Epoch [950/1000], Loss: 0.0820\n",
      "Epoch [955/1000], Loss: 0.0820\n",
      "Epoch [960/1000], Loss: 0.0820\n",
      "Epoch [965/1000], Loss: 0.0819\n",
      "Epoch [970/1000], Loss: 0.0819\n",
      "Epoch [975/1000], Loss: 0.0819\n",
      "Epoch [980/1000], Loss: 0.0819\n",
      "Epoch [985/1000], Loss: 0.0819\n",
      "Epoch [990/1000], Loss: 0.0819\n",
      "Epoch [995/1000], Loss: 0.0819\n",
      "Epoch [1000/1000], Loss: 0.0819\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = x_train\n",
    "    targets = y_train\n",
    "    \n",
    "    #Forward + Backward + Optimize\n",
    "    \n",
    "    #clears the gradients of all optimized torch.Tensors \n",
    "    optimizer.zero_grad()\n",
    "    #applies linear transformation (a call to model.forward(inputs) )\n",
    "    output = model(inputs)\n",
    "    #calculates RMS error between targets and output\n",
    "    loss = criterion(output, targets)\n",
    "    #accumulates the gradient for each parameter\n",
    "    loss.backward()\n",
    "    #updates parameters based on current gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data[0])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHLxJREFUeJzt3X90VNW99/H3lwCGn6IRuWrIj2pUAoEUgoBorxpt0aXS\nonjBPLjkaZuH+qPeu6zVp/jUdiF99Nqr1y6hrFQt9VmpXLWieK+trVehVsQSbBATFEEDBlyKKCoG\nhZD9/DEhDTHJnJmcM3PmzOe1FmuYMztn9iHMZ/bZZ5+9zTmHiIhES790V0BERPyncBcRiSCFu4hI\nBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIR1D9db3zccce5oqKidL29iEhG2rBhwwfO\nuZHxyqUt3IuKiqirq0vX24uIZCQz2+6lnLplREQiSOEuIhJBCncRkQhSuIuIRFDccDezB83sfTN7\nrYfXzcx+YWZbzexVM5vofzVFRCQRXlruy4EZvbx+IVDS/qca+GXfqyWRVVsLRUXQr1/ssbY23TUS\niaS44e6c+zPwYS9FZgIPuZh1wAgzO8GvCkqE1NZCdTVs3w7OxR6rqxXwIgHwo8/9JOCdTs+b27d9\niZlVm1mdmdXt3r3bh7eWjLJwIbS0HLmtpSW2XUR8ldILqs65GudchXOuYuTIuDdYSdTs2JHYdhFJ\nmh/hvhMY3el5fvs2kSMVFCS2XUSS5ke4rwKuah81MxX42Dn3rg/7lahZvBgGDz5y2+DBse0i4qu4\nc8uY2cPAOcBxZtYM3AYMAHDOLQOeBi4CtgItwPygKisZrqoq9rhwYawrpqAgFuyHt4uIb8w5l5Y3\nrqiocKGYOKy2VmEjIhnDzDY45yrilUvbrJChcHho3uERHIeH5oECXkQyWnZPP6CheSISUdkd7hqa\nJyIRld3hrqF5IhJR2R3uGponIhDJOY+yO9yrqqCmBgoLwSz2WFOji6ki2SSicx5pKKSIZLeiolig\nd1VYCE1Nqa5NXF6HQmZ3y11EJKIDKxTuIpLdIjqwQuEuItktogMrFO4ikt0iOrAiu6cfEBGBWJBn\neJh3pZa7iEgEKdxFRCJI4S4iEkEKd8kOEby9XKQ3uqAq0ad5+yULqeUu0ad5+yULKdwl+iJ6e7lI\nbxTuEn0Rvb1cpDcKd4m+iN5eLtIbhbtEX0RvLxfpjUbLSHaI4O3lIr1Ry11EMofuV/BMLXcRyQy6\nXyEharmLJEItx/TR/QoJUctdxCu1HNNL9yskRC13Ea/Uckwv3a+QEIW7iFdqOaaX7ldIiMJdxCu1\nHNNL9yskROEu4pVajulXVQVNTdDWFntUsPfIU7ib2Qwze8PMtprZLd28frSZPWVmG82swczm+19V\nkTRTy1EySNxwN7McYAlwIVAKzDWz0i7FrgUanXMTgHOAfzOzgT7X1V8a0ibJUMtRMoSXoZBnAFud\nc28BmNkKYCbQ2KmMA4aZmQFDgQ+BVp/r6h8NaRORiPPSLXMS8E6n583t2zq7DxgD7AI2ATc459p8\nqWEQNKRNRCLOrwuq3wDqgROBcuA+MxvetZCZVZtZnZnV7d6926e3ToKGtIlIxHkJ953A6E7P89u3\ndTYfeNzFbAXeBk7vuiPnXI1zrsI5VzFy5Mhk69x3GtImIhHnJdzXAyVmVtx+kXQOsKpLmR1AJYCZ\njQJOA97ys6K+0pA2EYm4uOHunGsFrgOeATYDjzjnGsxsgZktaC+2CDjTzDYB/w3c7Jz7IKhK95mG\ntIlIxJlzLi1vXFFR4erq6tLy3iIimcrMNjjnKuKV0x2qIiIRpHAXEYkghbuISAQp3EVEIkjhLiIS\nQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJd\nRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIuFVWwtFRdCvX+yxtjbdNcoY/dNdARGRbtXWQnU1\ntLTEnm/fHnsOUFWVvnplCLXcRSScFi78e7Af1tIS2y5xKdxFJJx27EhsuxxB4R4l6p+UKCkoSGy7\nHEHhHhWH+ye3bwfn/t4/qYCXTLV4MQwefOS2wYNj2yUuhXtUqH9SoqaqCmpqoLAQzGKPNTW6mOqR\nOefS8sYVFRWurq4uLe8dSf36xVrsXZlBW1vq6yMigTCzDc65injl1HKPCvVPikgn0Q73bLrAqP5J\nEekkuuGebRcY1T8pIp1Et8+9qCgW6F0VFkJTU3DvKyISIF/73M1shpm9YWZbzeyWHsqcY2b1ZtZg\nZmsSrbDvdAOEiGSxuOFuZjnAEuBCoBSYa2alXcqMAJYClzrnxgKzA6hrYnSBMXyy6RqISJp5abmf\nAWx1zr3lnDsArABmdilzJfC4c24HgHPufX+rmQRdYAyXbLsGIpJmXsL9JOCdTs+b27d1dipwjJmt\nNrMNZnaVXxVMmi4whotushJJKb+m/O0PTAIqgUHAS2a2zjm3pXMhM6sGqgEKUtE9UlWlMA8LXQMR\nSSkvLfedwOhOz/Pbt3XWDDzjnPvMOfcB8GdgQtcdOedqnHMVzrmKkSNHJltnyUS6BiKSUl7CfT1Q\nYmbFZjYQmAOs6lLmSeAsM+tvZoOBKcBmf6sqGU3XQERSKm64O+dageuAZ4gF9iPOuQYzW2BmC9rL\nbAb+ALwK/BW43zn3WnDVloyjayAiKRXdm5hERCJIE4eJiGQxhbuISAQp3EVEIkjhLiISQQp3EZEI\nUriLiESQwl1EJIIU7iIiEeTXxGEps2ffF5x/9xo+ajnYsS1vyECumlbE3CmjOX5YbhprJyISDhkX\n7i0HDh0R7AB7PjvAPc9u4Z5nt/TwUzF5QwYyb1oh159XQk4/C7KaIiJplbHTDzjnePP9ffzrH17n\n2c3+rw3y2+9M4cxTjvN9vyIifeF1+oGMDXevdu7dzy+efZP/qHsnfuEElZ10NI9fcyYDcnTpQkRS\nQ+GegNZDbfzLIxt5auOuQPZfM28SXx/7D4HsW0Syi8I9AGu3fcCVv3o5sP2/cfsMjuqfE9j+RSTz\nKdzTpPVQG/9Us44N2z8KZP93XzGBWRPzA9m3iISfwj3knm18j+88FNzx6yxAJJoU7hFw8FAbJQt/\nH9j+/232BC6bpLMAkUyicM8iT23cxfUP/y2w/essQCQ8FO5yhKDPAu66fDyzK0YHtn8RiVG4S1L+\n69V3ufa3rwS2/9cXzSB3gM4CRJKlcJfABH0W8K+XjeeKyToLEOmOwl3S7ulN73JNrc4CRPykcJeM\n0HqojVMCPAu487Iy/mlyQWD7F0k1hbtEyu83vcv3AjoLOP0fhrHqurMY2F9zBEn4Kdwl67QeauO0\n//MHDrUF83/61/Mnc+5pxweybxGvFO4iPVjf9CGzl70UyL5PGzWMVddP130BEhiFu0htLSxcCDt2\nQEEBLF4MVVWefvRQm+PqX/+VF978IJCq/frqyZx7us4CJHEKd8lutbVQXQ0tLX/fNngw1NR4Dngv\n6po+5PKAzgJOHTWUp64/S2cBcgSFu2S3oiLYvv3L2wsLoakppVU51Ob4n8vXs2bL7kD2/x/VU5ny\nlbxA9i3ho3CX7NavH3T3f9sM2tpSXx8PNmz/kMt+GcxZwAWlo1hy5USNCIoAhbtktxC13P10qM3x\n7d+sZ/UbwZwFrKieylSdBYSa13Dv73FnM4B7gRzgfufcHT2Umwy8BMxxzj2WQH1F/LV4cfd97osX\np69OPsjpZyyff0bccpvf/YQL730h4f3PqVkXt4zOAvqgDxf5ExW35W5mOcAW4AKgGVgPzHXONXZT\n7k/A58CD8cJdLfdepPA/QKTp37FXbW2O21Y18P/WdXOG44OHvzuVaSfrLKCDTxf5feuWMbNpwE+c\nc99of/6/AZxz/7dLuX8GDgKTgf9UuCcpRaM8RLxK9izAi//1ta/wg2+cxoCcLDgL8Kmr0M9wvxyY\n4Zz7TvvzecAU59x1ncqcBPwWOBd4EIV78iLaVyzR1tbm+OlTDfzmJf/PAvKGDGT5/DMoyz/a932n\nlE8X+X3tc/fg34GbnXNtZtZbpaqBaoCCAk3m1K0dOxLbLhIC/foZP505jp/OHNdruS3vfco/r6in\n8d1PPO97z2cHuOS+v8Qtd/OM0/nO2cXhPQsoKOi+4RZQFvrSLWNmbwOHU/04oAWods490dN+Q9Vy\nD1PfrFrukuXa2hwPvvg2t//XZl/3O/So/lw1rZD/MbWQE0cM8nXfnoSwz70/sQuqlcBOYhdUr3TO\nNfRQfjmZ1C0Ttj7usNVHJKR27t3PbU828Ozm93zb57yphXy1YAQTRo+gOG8I/fr13BORFB8akr6O\nczezi4h1veQQGwmz2MwWADjnlnUpu5ygwj2IFnYYW8phOpMQyWDOOV7atoffvNTEMw19/xIYNfwo\nJuTHwr989AjK8o9meO6Avlc0AdG7iSmoFm0G3skoIv768LMDrHtrD7v27mdj88dsfGcvOz5sif+D\nPZgwegTl+UczYbT/ZwHRC/egWthhbLmLSOg452j+aD/17+zl1ea9HV8CX7Qm3gj80798jZJRw5Kq\nR6pHywQvqFEkEb2TUUT8ZWaMPnYwo48dzCUTTuyx3P4Dh2jY9TH17+zt8Szg3Y8/TzrcvcqccO/r\nMKKe+rEPd+moj1tEfDBoYA4VRcdSUXRsWusR0gGh3Vi8ONai7sxrC/twf/327bH+9e3bY89ra2Ov\nV1XFumDa2mKPCnYRyXCZE+5VVbGLp4WFsYudhYXeL6YuXHhktwvEni9cGExdw6K2NnZNoV+/2OPh\nLzMRibzMCXdIvoUd5rs+gwrgeGcr4h99iUoIZVa4J6unfvl0T4EQZABn69lKqulLVEIqO8K9L/31\nQQoygMN8thIl+hKVkMqOcO9Lf32QggzgsJ6tRI2+RCWksiPcIZwjYoIM4LCerUSNvkQlpLIn3MMo\nyAAO69lK1OhLVEJK4Z5OQQdwGM9WokZfohJSmTO3jIiIeJ5bRi13EZEIUriLiESQwl1EJIIyN9x1\ny7dIZtFnNqUyM9x1y7f0RiESPvrMplxmjpbR6knSEy0wHk76zPom2qNldMu39ERzvYRTqj+zOnvL\n0HDXLd/ZJZEPqr74wymVn1l1AQGZGu665Tt7JPpB1Rd/OKXyM6uzNyBTw123fGePRD+o+uIPp1R+\nZnX2BmTqBVXJHv36xVrsXZnF5szpTk+LoUt2iPjF22hfUJXskUw3iyZMy246ewMU7hJ2+qBKotRt\nC0D/dFdApFeHP5DqZpFEVFVl/f8RhbuEnz6oIglTt4yISAQp3EVEIkjhLiISQQp3EZEI8hTuZjbD\nzN4ws61mdks3r1eZ2atmtsnM1prZBP+rKiIiXsUNdzPLAZYAFwKlwFwzK+1S7G3gH51zZcAioMbv\nioqIiHdeWu5nAFudc2855w4AK4CZnQs459Y65z5qf7oOyPe3miIikggv4X4S8E6n583t23rybeD3\nfamUiIj0ja83MZnZucTC/aweXq8GqgEKNAWriEhgvLTcdwKjOz3Pb992BDMbD9wPzHTO7eluR865\nGudchXOuYuTIkcnUV0REPPAS7uuBEjMrNrOBwBxgVecCZlYAPA7Mc85t8b+aEnpa1kwkVOKGu3Ou\nFbgOeAbYDDzinGswswVmtqC92I+BPGCpmdWbmSZqzyaJrJakLwGRlNBiHdJ3XhdHOPwl0HllpcGD\ns3I6VpFkabGOdMnGlqnXZc20tqVIyijc/ZStq657XS1Ja1uKpIzC3U/Z2jL1ulpSMkvmiUhStFiH\nn7K1Zep1taTFi7vvc9eSeaFw8OBBmpub+fzzz9NdFQFyc3PJz89nwIABSf28wt1PBQXdX1jMhpap\nl9WStGReqDU3NzNs2DCKioows3RXJ6s559izZw/Nzc0UFxcntQ91y/hJiznHV1UVG0HT1hZ7VLCH\nxueff05eXp6CPQTMjLy8vD6dRSnc/aRV1yXDKdjDo6+/C4W739QyFUlac3MzM2fOpKSkhJNPPpkb\nbriBAwcOdFt2165dXH755XH3edFFF7F3796k6vOTn/yEn//853HLDR06tNfX9+7dy9KlS5OqQ7IU\n7iKSHJ/v6XDOMWvWLL75zW/y5ptvsmXLFvbt28fCbkabtba2cuKJJ/LYY4/F3e/TTz/NiBEj+lS3\nvlK4i0hmCOCejueee47c3Fzmz58PQE5ODvfccw8PPvggLS0tLF++nEsvvZTzzjuPyspKmpqaGDdu\nHAAtLS1cccUVlJaW8q1vfYspU6Zw+A74oqIiPvjgA5qamhgzZgzf/e53GTt2LF//+tfZv38/AL/6\n1a+YPHkyEyZM4LLLLqOl65DmLt5++22mTZtGWVkZt956a8f2ffv2UVlZycSJEykrK+PJJ58E4JZb\nbmHbtm2Ul5dz00039VjOTwp3EUlcAPd0NDQ0MGnSpCO2DR8+nIKCArZu3QrAK6+8wmOPPcaaNWuO\nKLd06VKOOeYYGhsbWbRoERs2bOj2Pd58802uvfZaGhoaGDFiBL/73e8AmDVrFuvXr2fjxo2MGTOG\nBx54oNe63nDDDXzve99j06ZNnHDCCR3bc3NzWblyJa+88grPP/88N954I8457rjjDk4++WTq6+u5\n6667eiznJ4W7iCQuTfd0XHDBBRx77LFf2v6Xv/yFOXPmADBu3DjGjx/f7c8XFxdTXl4OwKRJk2hq\nn/votdde4+yzz6asrIza2loaGhp6rceLL77I3LlzAZg3b17HduccP/rRjxg/fjznn38+O3fu5L33\n3vvSz3st1xcKdxFJXAB3G5eWln6pxf3JJ5+wY8cOTjnlFACGDBmS9P4BjjrqqI6/5+Tk0NraCsDV\nV1/Nfffdx6ZNm7jttts8DUHsbjRLbW0tu3fvZsOGDdTX1zNq1Khu9+W1XF8o3EUkcQHc01FZWUlL\nSwsPPfQQAIcOHeLGG2/k6quvZnDX9+pi+vTpPPLIIwA0NjayadOmhN77008/5YQTTuDgwYPUerhu\nMH36dFasWAFwRPmPP/6Y448/ngEDBvD888+zvf2mxmHDhvHpp5/GLecnhbuIJC6AezrMjJUrV/Lo\no49SUlLCqaeeSm5uLj/72c/i/uw111zD7t27KS0t5dZbb2Xs2LEcffTRnt970aJFTJkyhenTp3P6\n6afHLX/vvfeyZMkSysrK2Lnz7wvTVVVVUVdXR1lZGQ899FDHvvLy8pg+fTrjxo3jpptu6rGcnzSf\nu4gAsHnzZsaMGZPuaiTl0KFDHDx4kNzcXLZt28b555/PG2+8wcCBA9NdtT7p7nfidT53zS0jIhmv\npaWFc889l4MHD+KcY+nSpRkf7H2lcBeRjDds2DDUE3Ak9bmLiESQwl1EJIIU7iIiEaRwFxGJIIW7\niIRGTk4O5eXlHX+ampqoq6vj+9//PgCrV69m7dq1HeWfeOIJGhsbE36fnqboPbzd63TCYabRMiIS\nGoMGDaK+vv6IbUVFRVRUxIZ1r169mqFDh3LmmWcCsXC/+OKLKS0t9bUeXqcTDjO13EUk1FavXs3F\nF19MU1MTy5Yt45577qG8vJw1a9awatUqbrrpJsrLy9m2bRvbtm1jxowZTJo0ibPPPpvXX38d6HmK\n3p50nk54+fLlzJo1ixkzZlBSUsIPf/jDjnJ//OMfmTZtGhMnTmT27Nns27cvmH+EJKjlLpKJamsD\nXWj8p0810LjrE9/2B1B64nBuu2Rsr2X279/fMWtjcXExK1eu7HitqKiIBQsWMHToUH7wgx8AcOml\nl3LxxRd3dKFUVlaybNkySkpKePnll7nmmmt47rnnOqboveqqq1iyZEnCda+vr+dvf/sbRx11FKed\ndhrXX389gwYN4vbbb+fZZ59lyJAh3Hnnndx99938+Mc/Tnj/QVC4i2SawwtlHJ5P/fBCGZDxyzp2\n1y3j1b59+1i7di2zZ8/u2PbFF18AsSl6D8/dPm/ePG6++eaE9l1ZWdkxV01paSnbt29n7969NDY2\nMn36dAAOHDjAtGnTkqp7EBTuIpmmt4UyfAr3eC3sMGpra2PEiBE9fjn0ZcHp7qYKds5xwQUX8PDD\nDye93yCpz10kHp/XCu2zNC2UEQZdp87t/Hz48OEUFxfz6KOPArEFMTZu3Aj0PEVvX0ydOpUXX3yx\nY5Wozz77jC1btviybz8o3EV6E8BaoX0WwEIZmeKSSy5h5cqVlJeX88ILLzBnzhzuuusuvvrVr7Jt\n2zZqa2t54IEHmDBhAmPHju1Ym7SnKXr7YuTIkSxfvpy5c+cyfvx4pk2b1nEBNww05a9Ib4qKYoHe\nVWEhtC/RlnJd+9whtlBGH+dTz+Qpf6OqL1P+emq5m9kMM3vDzLaa2S3dvG5m9ov21181s4meay8S\nZmHsAglgoQyJnrgXVM0sB1gCXAA0A+vNbJVzrvNtYRcCJe1/pgC/bH8UyWwFBd233NPdBVJVpTCX\nXnlpuZ8BbHXOveWcOwCsAGZ2KTMTeMjFrANGmNkJPtdVJPUCWCtUJBW8hPtJwDudnje3b0u0jEjm\nybIukHRdg5Mv6+vvIqXj3M2sGqgGKEj3aa2IV1nSBZKbm8uePXvIy8vr05hw6TvnHHv27CE3Nzfp\nfXgJ953A6E7P89u3JVoG51wNUAOx0TIJ1VREApWfn09zczO7d+9Od1WE2Jdtfn5+0j/vJdzXAyVm\nVkwssOcAV3Ypswq4zsxWELuQ+rFz7t2kayUiKTdgwACKi4vTXQ3xSdxwd861mtl1wDNADvCgc67B\nzBa0v74MeBq4CNgKtADzg6uyiIjE46nP3Tn3NLEA77xtWae/O+Baf6smIiLJ0vQDIiIRlLbpB8xs\nN9DN3SG9Og74IIDqZAIde/bK5uPXsX9ZoXNuZLwfTlu4J8PM6rzMqRBFOvbsPHbI7uPXsSd/7OqW\nERGJIIW7iEgEZVq416S7AmmkY89e2Xz8OvYkZVSfu4iIeJNpLXcREfEglOGezYuDeDj2qvZj3mRm\na81sQjrqGYR4x96p3GQzazWzy1NZvyB5OXYzO8fM6s2swczWpLqOQfHwf/5oM3vKzDa2H3tk7oA3\nswfN7H0ze62H15PPOudcqP4Qm+JgG/AVYCCwESjtUuYi4PeAAVOBl9Nd7xQe+5nAMe1/vzCbjr1T\nueeI3TF9ebrrncLf+wigEShof358uuudwmP/EXBn+99HAh8CA9Ndd5+O/2vAROC1Hl5POuvC2HLP\n5sVB4h67c26tc+6j9qfriM3AGQVefu8A1wO/A95PZeUC5uXYrwQed87tAHDOReX4vRy7A4ZZbB7i\nocTCvTW11QyGc+7PxI6nJ0lnXRjDPZsXB0n0uL5N7Fs9CuIeu5mdBHyL2DKOUeLl934qcIyZrTaz\nDWZ2VcpqFywvx34fMAbYBWwCbnDOtaWmemmXdNaldLEO8Y+ZnUss3M9Kd11S6N+Bm51zbVm4mER/\nYBJQCQwCXjKzdc65LemtVkp8A6gHzgNOBv5kZi845z5Jb7XCLYzh7tviIBnI03GZ2XjgfuBC59ye\nFNUtaF6OvQJY0R7sxwEXmVmrc+6J1FQxMF6OvRnY45z7DPjMzP4MTAAyPdy9HPt84A4X64TeamZv\nA6cDf01NFdMq6awLY7dMx+IgZjaQ2OIgq7qUWQVc1X4leSrRWRwk7rGbWQHwODAvYq22uMfunCt2\nzhU554qAx4BrIhDs4O3//JPAWWbW38wGE1sUZ3OK6xkEL8e+g9gZC2Y2CjgNeCultUyfpLMudC13\nl8WLg3g89h8DecDS9hZsq4vAxEoejz2SvBy7c26zmf0BeBVoA+53znU7fC6TePy9LwKWm9kmYqNG\nbnbORWKmSDN7GDgHOM7MmoHbgAHQ96zTHaoiIhEUxm4ZERHpI4W7iEgEKdxFRCJI4S4iEkEKdxGR\nCFK4i4hEkMJdRCSCFO4iIhH0/wH1d+im8L6NOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1095eb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot\n",
    "predicted = model(x_train).data\n",
    "plt.plot(x_train.numpy(), y_train.numpy(), 'ro', label='Original data')\n",
    "plt.plot(x_train.numpy(), predicted.numpy(), label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
